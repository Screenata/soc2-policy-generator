# Evidence Collection Scripts — Templates & Conventions

Pre-built scripts for 21 common SaaS tools ship in `assets/scripts/`. The agent copies them to `.compliance/scripts/`, adds a config file, and tests. If a pre-built script doesn't work (API changed, version mismatch), the agent falls back to generating a new script on demand.

## Directory Structure

```
.compliance/scripts/
├── collect-all.sh            # Runner: executes all *.sh scripts in directory
├── okta.sh                   # (generated per user's SaaS stack)
├── okta.config.json          # { "domain": "https://company.okta.com" }
├── pagerduty.sh
├── pagerduty.config.json     # {}
├── jira.sh
├── jira.config.json          # { "domain": "https://company.atlassian.net", "project": "ENG" }
├── aws.sh                    # (generated per cloud provider)
├── aws.config.json           # { "region": "us-east-1" }
├── ...
```

## Per-Tool Config Files

Each tool gets its own `{tool}.config.json` co-located with its script.

**Rules:**
- Non-secret settings only — secrets stay in env vars / GitHub Secrets
- Adding a tool = generate script + config (atomic pair)
- Removing a tool = delete script + config (atomic pair)
- Config file is committed to the repo alongside the script
- If a tool needs no config, create an empty JSON object: `{}`

**Example configs:**
```json
// okta.config.json
{ "domain": "https://company.okta.com" }

// jira.config.json
{ "domain": "https://company.atlassian.net", "project": "ENG" }

// aws.config.json
{ "region": "us-east-1" }

// datadog.config.json
{ "site": "datadoghq.com" }

// pagerduty.config.json
{}
```

---

## Script Template

Every evidence collection script follows this pattern:

```bash
#!/usr/bin/env bash
# .compliance/scripts/{tool}.sh
# SOC 2 evidence collection for {Tool Name}
# Requires: {TOOL}_API_TOKEN env var
# Config:   {tool}.config.json (co-located)
set -euo pipefail

# ── Config ──────────────────────────────────────────────
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
CONFIG="${SCRIPT_DIR}/$(basename "$0" .sh).config.json"

# Read config values (tool-specific)
DOMAIN=$(jq -r '.domain // empty' "$CONFIG")

# ── Output setup ────────────────────────────────────────
OUT="${SOC2_EVIDENCE_DIR:-.compliance/evidence/saas}/$(basename "$0" .sh)-evidence.md"
mkdir -p "$(dirname "$OUT")"

{
  echo "# {Tool Name} - SaaS Evidence"
  echo ""
  echo "> Scan date: $(date -u '+%Y-%m-%d %H:%M UTC')"
  echo "> Tool: {Tool Name}"
  echo ""
  echo "| Control | Extracted Value | Tool | API Endpoint | Raw Evidence |"
  echo "|---------|----------------|------|-------------|-------------|"
} > "$OUT"

# ── API calls ───────────────────────────────────────────
# Each call: fetch → parse → write evidence row
# Use || echo "{}" for graceful degradation on API errors

result=$(curl -sf -H "Authorization: {auth_scheme} ${API_TOKEN}" \
  "${DOMAIN}/{endpoint}" || echo "{}")

value=$(echo "$result" | jq -r '{extraction_expression}')
echo "| {control_name} | **${value}** | {Tool} | \`{endpoint}\` | $(echo "$result" | jq -c '{summary}' | head -c 80) |" >> "$OUT"

# ── Footer ──────────────────────────────────────────────
echo "" >> "$OUT"
echo "*Auto-generated by soc2-evidence scripts.*" >> "$OUT"

echo "OK: {tool} evidence written to $OUT"
```

**Key patterns:**
- `set -euo pipefail` — fail fast, but individual API calls use `|| echo "{}"` for graceful degradation
- Config read with `jq -r '.key // empty'` — the `// empty` prevents null output
- Output path derived from script name: `$(basename "$0" .sh)-evidence.md`
- `SOC2_EVIDENCE_DIR` env var overrides default output directory
- Final `echo "OK: ..."` line confirms successful execution for the agent to verify

---

## collect-all.sh Runner

Discovers and runs all scripts automatically — no central registry needed:

```bash
#!/usr/bin/env bash
# .compliance/scripts/collect-all.sh
# Runs all evidence collection scripts in this directory
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
echo "SOC 2 Evidence Collection — $(date -u '+%Y-%m-%d %H:%M UTC')"
echo "================================================"

failed=0
for script in "${SCRIPT_DIR}"/*.sh; do
  [ "$(basename "$script")" = "collect-all.sh" ] && continue
  tool=$(basename "$script" .sh)
  echo ""
  echo "--- Collecting: ${tool} ---"
  if bash "$script"; then
    echo "OK: ${tool}"
  else
    echo "FAILED: ${tool} (continuing...)"
    failed=$((failed + 1))
  fi
done

echo ""
echo "================================================"
echo "Collection complete. ${failed} failure(s)."
[ "$failed" -eq 0 ] || exit 1
```

**Behavior:**
- Skips itself (`collect-all.sh`)
- Runs each `*.sh` script in the directory
- Reports pass/fail per script, continues on failure
- Exits with non-zero if any script failed

---

## Pre-built Scripts

21 pre-built scripts ship in `assets/scripts/`. The agent copies them to `.compliance/scripts/`, adds a config file, and tests. If a pre-built script doesn't work (API changed, version mismatch), the agent falls back to generating a new script on demand.

| Category | Tool | Script | Config | Secrets Required |
|----------|------|--------|--------|-----------------|
| Identity | Okta | `okta.sh` | `{ "domain": "https://company.okta.com" }` | `OKTA_API_TOKEN` |
| Identity | Auth0 | `auth0.sh` | `{ "domain": "company.auth0.com" }` | `AUTH0_CLIENT_ID`, `AUTH0_CLIENT_SECRET` |
| Identity | JumpCloud | `jumpcloud.sh` | `{}` | `JUMPCLOUD_API_KEY` |
| Monitoring | Datadog | `datadog.sh` | `{ "site": "datadoghq.com" }` | `DATADOG_API_KEY`, `DATADOG_APP_KEY` |
| Monitoring | PagerDuty | `pagerduty.sh` | `{}` | `PAGERDUTY_API_TOKEN` |
| Monitoring | New Relic | `newrelic.sh` | `{ "account_id": "123456" }` | `NEWRELIC_API_KEY` |
| Monitoring | Splunk | `splunk.sh` | `{ "instance": "company" }` | `SPLUNK_TOKEN` |
| Project Mgmt | Jira | `jira.sh` | `{ "domain": "https://company.atlassian.net", "project": "ENG" }` | `JIRA_API_TOKEN`, `JIRA_EMAIL` |
| Project Mgmt | Linear | `linear.sh` | `{}` | `LINEAR_API_KEY` |
| Project Mgmt | GitHub | `github.sh` | `{ "repo": "org/repo" }` | `GH_TOKEN` |
| Communications | Slack | `slack.sh` | `{}` | `SLACK_BOT_TOKEN` |
| Communications | Opsgenie | `opsgenie.sh` | `{}` | `OPSGENIE_API_KEY` |
| Communications | Statuspage | `statuspage.sh` | `{ "page_id": "abc123" }` | `STATUSPAGE_API_KEY` |
| HR | BambooHR | `bamboohr.sh` | `{ "subdomain": "company" }` | `BAMBOOHR_API_KEY` |
| HR | Gusto | `gusto.sh` | `{ "company_id": "123456" }` | `GUSTO_API_TOKEN` |
| HR | Rippling | `rippling.sh` | `{}` | `RIPPLING_API_TOKEN` |
| Endpoint | Jamf | `jamf.sh` | `{ "url": "https://company.jamfcloud.com" }` | `JAMF_CLIENT_ID`, `JAMF_CLIENT_SECRET` |
| Endpoint | Kandji | `kandji.sh` | `{ "subdomain": "company" }` | `KANDJI_API_TOKEN` |
| Endpoint | Intune | `intune.sh` | `{}` | `INTUNE_CLIENT_ID`, `INTUNE_CLIENT_SECRET`, `INTUNE_TENANT_ID` (token acquired via OAuth2) |
| Security | Snyk | `snyk.sh` | `{ "org_id": "abc-123" }` | `SNYK_TOKEN` |
| Security | SonarCloud | `sonarcloud.sh` | `{ "project_key": "org_project", "base_url": "https://sonarcloud.io" }` | `SONAR_TOKEN` |

Plus `collect-all.sh` — the runner that discovers and executes all scripts in the directory.

---

## Test-First Workflow

The agent follows this workflow for each tool:

### Step 1: Copy or generate script + config

For each tool the user selected:
- **If pre-built script exists** in `assets/scripts/{tool}.sh`: copy it to `.compliance/scripts/{tool}.sh`
- **If no pre-built script**: generate one using API patterns from the relevant `references/saas-integrations/{category}.md`
- Generate `.compliance/scripts/{tool}.config.json` with non-secret config values

The agent asks the user for config values (domain, project key, region, etc.) and writes them to the config file.

### Step 2: User adds secrets

The agent asks the user to add the required API token to `.compliance/secrets.env`:
```
Please add your API token to .compliance/secrets.env:
  OKTA_API_TOKEN=your-token-here
```

The `.compliance/secrets.env` file uses standard `KEY=value` format (one per line, no `export` prefix needed). The file is auto-loaded by `collect-all.sh` and should be added to `.gitignore`. Example:
```env
# .compliance/secrets.env — local testing only, do NOT commit
OKTA_API_TOKEN=00abc123...
OKTA_DOMAIN=https://company.okta.com
DATADOG_API_KEY=abc123...
DATADOG_APP_KEY=def456...
```

For individual script testing, the agent sources the file before running:
```bash
set -a; source .compliance/secrets.env; set +a
bash .compliance/scripts/okta.sh
```

### Step 3: Test locally

The agent runs the script:
```bash
bash .compliance/scripts/okta.sh
```

### Step 4: Verify and iterate

The agent inspects the output:
- Reads the generated evidence file
- Checks for errors (empty values, API failures, malformed JSON)
- Fixes issues in the script
- Reruns until the output is correct

### Step 5: Wire into workflow

Once the script is verified, the agent generates or updates the GitHub Actions workflow to call it:
```yaml
- name: "Scan: Okta"
  env:
    OKTA_DOMAIN: ${{ secrets.OKTA_DOMAIN }}
    OKTA_API_TOKEN: ${{ secrets.OKTA_API_TOKEN }}
  run: |
    if [ -z "$OKTA_API_TOKEN" ]; then echo "Skipping Okta (no token)"; exit 0; fi
    bash .compliance/scripts/okta.sh
```

---

## Workflow Wiring Pattern

### SaaS tools

Each tool step in the workflow:
```yaml
- name: "Scan: {Tool Name}"
  env:
    {TOOL}_API_TOKEN: ${{ secrets.{TOOL}_API_TOKEN }}
    {TOOL}_DOMAIN: ${{ secrets.{TOOL}_DOMAIN }}    # if needed
  run: |
    if [ -z "$TOOL_API_TOKEN" ]; then echo "Skipping {tool} (no token)"; exit 0; fi
    # Note: replace TOOL with actual tool name, e.g. $OKTA_API_TOKEN
    bash .compliance/scripts/{tool}.sh
```

### Cloud providers

```yaml
- name: "Scan: AWS"
  env:
    AWS_REGION: ${{ inputs.region || 'us-east-1' }}
  run: bash .compliance/scripts/aws.sh
```

### Code scanning

```yaml
- name: "Scan: Code patterns"
  run: bash .compliance/scripts/code-scan.sh
```

### collect-all.sh (alternative)

Instead of individual steps, the workflow can call the runner:
```yaml
- name: "Collect all evidence"
  env:
    OKTA_API_TOKEN: ${{ secrets.OKTA_API_TOKEN }}
    PAGERDUTY_API_TOKEN: ${{ secrets.PAGERDUTY_API_TOKEN }}
    # ... all secrets
  run: bash .compliance/scripts/collect-all.sh
```

This is simpler but gives less granular failure visibility in the workflow UI. Prefer individual steps for production workflows.

---

## Cloud Provider Script Template

Cloud scripts follow the same pattern but use CLI tools instead of curl:

```bash
#!/usr/bin/env bash
# .compliance/scripts/aws.sh
# SOC 2 evidence collection for AWS
# Requires: AWS credentials configured (env vars or AWS CLI profile)
# Config:   aws.config.json (co-located)
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
CONFIG="${SCRIPT_DIR}/$(basename "$0" .sh).config.json"
REGION=$(jq -r '.region // "us-east-1"' "$CONFIG")

OUT="${SOC2_EVIDENCE_DIR:-.compliance/evidence/cloud}/aws-evidence.md"
mkdir -p "$(dirname "$OUT")"

{
  echo "# AWS Cloud Infrastructure Evidence"
  echo ""
  echo "> Scan date: $(date -u '+%Y-%m-%d %H:%M UTC')"
  echo "> Region: ${REGION}"
  echo ""
  echo "| Control | Extracted Value | Service | Region | Command | Raw Evidence |"
  echo "|---------|----------------|---------|--------|---------|-------------|"
} > "$OUT"

# IAM Password Policy
result=$(aws iam get-account-password-policy --output json 2>&1 || echo '{"error": true}')
if echo "$result" | jq -e '.PasswordPolicy' > /dev/null 2>&1; then
  min_len=$(echo "$result" | jq -r '.PasswordPolicy.MinimumPasswordLength')
  echo "| Password min length | **${min_len} characters** | IAM | global | \`aws iam get-account-password-policy\` | MinimumPasswordLength: ${min_len} |" >> "$OUT"
fi

# (additional scans per aws.md scanning patterns...)

echo "" >> "$OUT"
echo "*Auto-generated by soc2-evidence scripts.*" >> "$OUT"
echo "OK: aws evidence written to $OUT"
```

---

## Code Scan Script Template

Code scanning scripts use grep/find instead of API calls:

```bash
#!/usr/bin/env bash
# .compliance/scripts/code-scan.sh
# SOC 2 evidence collection from codebase patterns
# No external credentials needed
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
REPO_ROOT="${SCRIPT_DIR}/../.."  # Adjust based on scripts/ location

OUT="${SOC2_EVIDENCE_DIR:-.compliance/evidence/code}/code-evidence.md"
mkdir -p "$(dirname "$OUT")"

{
  echo "# Code Evidence"
  echo ""
  echo "> Scan date: $(date -u '+%Y-%m-%d %H:%M UTC')"
  echo "> Git SHA: $(git -C "$REPO_ROOT" rev-parse --short HEAD 2>/dev/null || echo 'unknown')"
  echo ""
  echo "| Control | Extracted Value | File | Line | Raw Evidence |"
  echo "|---------|----------------|------|------|-------------|"
} > "$OUT"

# Auth middleware detection
grep -rnE 'withAuth|requireAuth|isAuthenticated|authMiddleware' \
  --include='*.ts' --include='*.js' "$REPO_ROOT/src/" 2>/dev/null \
  | head -5 \
  | while IFS=: read -r file line match; do
      echo "| Auth middleware | **detected** | ${file#$REPO_ROOT/} | $line | \`$(echo "$match" | head -c 60)\` |"
    done >> "$OUT"

# (additional patterns from scanning-patterns/*.md...)

echo "" >> "$OUT"
echo "*Auto-generated by soc2-evidence scripts.*" >> "$OUT"
echo "OK: code evidence written to $OUT"
```
